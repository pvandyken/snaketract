{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pickle\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "import subprocess as sp\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import more_itertools as itx\n",
    "import multiprocessing as mp\n",
    "import tqdm\n",
    "import attrs\n",
    "# import sklearn\n",
    "import shutil\n",
    "import plotly.express as px\n",
    "import functools as ft\n",
    "from typing import Any, List\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import IPython.display as D\n",
    "import imageio.v3 as iio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rc('ytick', labelsize=11, color=\"#101010\")\n",
    "plt.rc('xtick', labelsize=11, color=\"#101010\")\n",
    "plt.rc(\"font\", family=\"Roboto\")\n",
    "plt.rc(\"figure\", titlesize=14, titleweight='bold', edgecolor='black', dpi=200, facecolor=\"white\")\n",
    "plt.rc(\"axes\", edgecolor=\"#a0a0a0\", titlecolor='#404040', facecolor=\"#f0f0f0\")\n",
    "plt.rc(\"axes.spines\", top=False, left=False, bottom=False, right=False)\n",
    "\n",
    "def letter_label(panel, letter, x=0, y=1, transform=None):\n",
    "    return panel.text(x, y, letter, transform=transform, size=20, weight='bold', color='#303030')\n",
    "\n",
    "from matplotlib import font_manager\n",
    "font_dirs = [Path.home() / '.fonts']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = Path(\"/scratch/knavynde/newtopsy\")\n",
    "def get_subj_metadata():\n",
    "    return (\n",
    "        pd.read_csv(ROOT / \"participants.tsv\", sep=\"\\t\")\n",
    "        .assign(subject=lambda df: df['participant_id'].map(lambda s: int(s[4:])))\n",
    "        .set_index(\"subject\")\n",
    "        .assign(group=lambda df: df[\"phenotype\"].map(cats))\n",
    "    )\n",
    "\n",
    "cats = {\n",
    "    \"3\": \"Treatment 3+ yr\",\n",
    "    \"CHR\": \"High risk\",\n",
    "    \"HC\": \"HC\",\n",
    "    \"FEP\": \"FEP\",\n",
    "    \"Control\": \"HC\",\n",
    "    \"Patient\": \"Patient\"\n",
    "}\n",
    "\n",
    "from bids import BIDSLayout\n",
    "\n",
    "def get_participants(participant_file):\n",
    "    with open(participant_file) as f:\n",
    "        subs = [match.group(1) for l in f.read().splitlines() if (match := re.match(r'sub-(.*)', l)) and match.group(1) != \"080\"]\n",
    "    return subs\n",
    "\n",
    "layout = BIDSLayout(ROOT, derivatives=True, database_path=ROOT / \".pybids\")\n",
    "layout_get = ft.partial(\n",
    "    layout.get,\n",
    "    subject=get_participants(ROOT / 'derivatives/snakedwi-0.1.0/participants.tsv'),\n",
    "    datatype=\"dwi\",\n",
    "    suffix=\"connectome\",\n",
    "    atlas=\"bn246\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from notebooks.utils import hex_to_rgb, get_lut, lut_label, titleize, distribution_plot, figures_to_html, plotly_tabulate, NbCache, underscore as __, error_line\n",
    "from notebooks.adjacency_matrix import group_outer, AdjacencyMatrix, filter_edges, community_sort\n",
    "from notebooks.bn_metadata import read_metadata\n",
    "from notebooks.graph_metrics import global_efficiency, local_efficiency\n",
    "from notebooks.plotly_helpers import plotly_grid, matplotlib_to_plotly\n",
    "nb_cache = NbCache(\"connectomics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_logile(bin: int, num_bins: int = 10):\n",
    "    def inner(matrix):\n",
    "        if bin >= num_bins:\n",
    "            raise ValueError(\"bin must be less then num_bins\")\n",
    "        masked = np.ma.masked_equal(matrix, 0)\n",
    "        log = np.ma.log10(masked)\n",
    "        threshold = 10 ** (((log.max() - log.min()) * bin / num_bins) + log.min())\n",
    "        return matrix < threshold\n",
    "    return inner\n",
    "\n",
    "            \n",
    "def community_sort(adj: AdjacencyMatrix):\n",
    "    components = nx.community.greedy_modularity_communities(\n",
    "        adj.graph, weight=\"weight\"\n",
    "    )\n",
    "    order = list(it.chain.from_iterable(components))\n",
    "    return adj.with_metadata(adj.metadata.reindex(index=np.array(order)))\n",
    "\n",
    "\n",
    "def metadata_ds():\n",
    "    participants = list(\n",
    "        map(int, get_participants(ROOT / 'derivatives/snakedwi-v0.1.0/participants.tsv'))\n",
    "    )\n",
    "    return xr.merge([\n",
    "        get_subj_metadata().loc[lambda df: df.index.isin(participants)].to_xarray(),\n",
    "        (\n",
    "            read_metadata()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"Label ID\": \"node\"})\n",
    "            .set_index(\"node\")\n",
    "            .to_xarray()\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "@attrs.frozen\n",
    "class Subject:\n",
    "    adj: AdjacencyMatrix\n",
    "    id: int\n",
    "    group: str\n",
    "    weight: str\n",
    "\n",
    "    @classmethod\n",
    "    def from_bids_entry(cls, __bids_entry, /, filter_level=1):\n",
    "        sub = int(__bids_entry.entities['subject'])\n",
    "        metadata = get_subj_metadata()\n",
    "        if not sub in metadata.index:\n",
    "            # print(f\"Dropped {sub} (Not found in metadata)\")\n",
    "            return None\n",
    "        adj = (\n",
    "            AdjacencyMatrix(\n",
    "                raw=np.genfromtxt(__bids_entry.path, delimiter=\",\"),\n",
    "                metadata=read_metadata(\"resources/brainnetome-regions.csv\"),\n",
    "            )\n",
    "            .mask_diagonal()\n",
    "            .mask_equal(0)\n",
    "            .mask_where(filter_logile(filter_level))\n",
    "            # .mask_where_meta(MetadataMasks.src_and_dest(\"hemisphere\").equals(\"L\"))\n",
    "        )\n",
    "        adj.props[\"distance\"] = np.ma.filled(1/adj.raw, np.NaN)\n",
    "        # dropped = drop(sub) if callable(drop) else drop\n",
    "        # df.drop(index=dropped, columns=dropped, inplace=True)\n",
    "        return cls(\n",
    "            adj = adj,\n",
    "            id = sub,\n",
    "            group = metadata.loc[sub, \"group\"],\n",
    "            weight = __bids_entry.entities[\"desc\"],\n",
    "        )\n",
    "    \n",
    "from matplotlib import cm\n",
    "spectral = matplotlib_to_plotly(cm.get_cmap('nipy_spectral'), 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Single Subject\n",
    "\n",
    "Here, test a single subject and view the connection density as a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adj = community_sort(\n",
    "    Subject.from_bids_entry(itx.one(layout_get(subject=\"001\", desc=\"avgFA\")))\n",
    "    .adj\n",
    "    .mask_where(filter_logile(0))\n",
    ")\n",
    "\n",
    "adj = adj.with_metadata(adj.metadata.sort_values([\"Lobe\", \"hemisphere\"]))\n",
    "# adj = adj.update(np.ma.log10(adj.raw))\n",
    "adj.plot(labels=[\"Lobe\", \"Long Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conmat(group):\n",
    "    adjs = []\n",
    "    md = get_subj_metadata()\n",
    "    for entry in layout_get(desc=\"sift2\"):\n",
    "        subj = Subject.from_bids_entry(entry)\n",
    "        if not subj:\n",
    "            continue\n",
    "        if md.loc[subj.id][\"group\"] != group:\n",
    "            continue\n",
    "        adj = subj.adj.mask_where(filter_logile(1))\n",
    "        adj = adj.with_metadata(adj.metadata.sort_values([\"Lobe\", \"hemisphere\"]))\n",
    "        adj = adj.update(np.ma.log10(adj.raw))\n",
    "        adjs.append(adj)\n",
    "\n",
    "    conmat = np.ma.empty((*adjs[0].masked.shape, len(adjs)))\n",
    "    for i, adj in enumerate(adjs):\n",
    "        conmat[..., i] = adj.masked\n",
    "    return conmat\n",
    "\n",
    "hc = get_conmat(\"HC\")\n",
    "pt = get_conmat(\"Patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = get_subj_metadata()\n",
    "subjects = [int(entry.get_entities()['subject']) for entry in layout_get(desc=\"sift2\")]\n",
    "part_ids = md.loc[subjects].sort_values([\"group\", \"participant_id\"])[\"participant_id\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.ma.dstack([hc, pt])\n",
    "groups = np.hstack([np.ones(hc.shape[-1]), np.zeros(pt.shape[-1])])\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(eigenvalues):\n",
    "    return sns.lineplot(x=np.arange(eigenvalues.shape[0]), y=eigenvalues)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "data = combined.filled(0)\n",
    "\n",
    "pca = PCA(n_components=10, svd_solver='randomized')\n",
    "pt_square = data.reshape(-1, data.shape[-1]).T\n",
    "L = pca.fit_transform(pt_square)\n",
    "print(pca.explained_variance_ratio_)\n",
    "df = pd.DataFrame(L)\n",
    "df['groups'] = groups\n",
    "df['id'] = part_ids\n",
    "comps = pca.components_[4].reshape(*data.shape[:2])\n",
    "\n",
    "# scree_plot(pca.singular_values_)\n",
    "# sns.heatmap(comps, cmap=\"vlag\")\n",
    "px.scatter_3d(df, x=2, y=3, z=4, color='groups', height=1000, hover_name='id')\n",
    "# sns.scatterplot(x=L[:, 0], y=L[:, 1], hue=groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "axes = fig.subplots(1, 2)\n",
    "\n",
    "def consensus_dist(conmat):\n",
    "    if np.ma.is_masked(conmat):\n",
    "        conmat = conmat.filled(0)\n",
    "    mean = np.ma.masked_equal(np.mean(conmat, -1), 0)\n",
    "    std = np.std(conmat, -1)\n",
    "    return np.ma.masked_where(np.median(conmat, -1) == 0, std / mean)\n",
    "\n",
    "\n",
    "sns.heatmap(consensus_dist(hc).filled(0), ax=axes[0])\n",
    "\n",
    "sns.heatmap(consensus_dist(pt).filled(0), ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection Density\n",
    "\n",
    "Density of connections as threshold increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c = np.genfromtxt(\n",
    "#     layout.get(\n",
    "#         subject=\"001\", datatype=\"dwi\", atlas=ATLAS, suffix=\"connectome\", desc=WEIGHT\n",
    "#     )[0].path, \n",
    "#     delimiter=\",\"\n",
    "# )\n",
    "def connection_density():\n",
    "    densities = []\n",
    "    for entry in layout_get(desc=\"sift2\"):\n",
    "        subj = Subject.from_bids_entry(entry)\n",
    "        if not subj:\n",
    "            continue\n",
    "        for i in range(10):\n",
    "            adj = (\n",
    "                subj\n",
    "                .adj\n",
    "                .mask_where(filter_logile(i))\n",
    "            )\n",
    "\n",
    "            densities.append((subj.id, i, nx.density(adj.graph)))\n",
    "    return pd.DataFrame(\n",
    "            densities,\n",
    "            columns=[\"subject\", \"threshold\", \"density\"]\n",
    "        ).set_index([\"subject\", \"threshold\"])\n",
    "densities = connection_density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.merge([\n",
    "    get_subj_metadata().to_xarray(),\n",
    "    densities.to_xarray(),\n",
    "])\n",
    "\n",
    "error_line(\n",
    "    __.pipe(\n",
    "        dict(\n",
    "            density=ds.groupby(\"group\").mean(\"subject\")[\"density\"],\n",
    "            std=ds.groupby(\"group\").std(\"subject\")[\"density\"],\n",
    "        ),\n",
    "        lambda _: xr.merge([_]).to_dataframe().reset_index()\n",
    "    ),\n",
    "    x=\"threshold\",\n",
    "    y=\"density\",\n",
    "    color=\"group\",\n",
    "    err=\"std\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Weight distribution plot\n",
    "bins = __.pipe(\n",
    "    adj.filled,\n",
    "    np.concatenate,\n",
    "    len,\n",
    "    np.sqrt,\n",
    "    np.arange,\n",
    ")\n",
    "# bins = (bins - np.min(bins))/np.ptp(bins)\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "# Distribution of raw weights\n",
    "rawdist = sns.histplot(adj.masked.flatten(), bins=bins, kde=False, ax=axes[0])\n",
    "rawdist.set(xlabel='Correlation Values', ylabel = 'Density Frequency')\n",
    "\n",
    "# Probability density of log10\n",
    "log10dist = sns.histplot(np.log10(adj.masked).flatten(), kde=False, ax=axes[1])\n",
    "log10dist.set(xlabel='log(weights)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Mass Function\n",
    "\n",
    "The total weight held by all edges of a given weight bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edge_mass_function():\n",
    "    cum_weight = []\n",
    "    for entry in layout_get(desc=\"sift2\"):\n",
    "        subj = Subject.from_bids_entry(entry)\n",
    "        if not subj:\n",
    "            continue\n",
    "        adj = (\n",
    "            subj\n",
    "            .adj\n",
    "            .mask_where(filter_logile(0))\n",
    "        )\n",
    "        arr = adj.masked.filled(0)\n",
    "        bins = __.pipe(\n",
    "            # arr,\n",
    "            # np.concatenate,\n",
    "            # len,\n",
    "            # np.sqrt,\n",
    "            246,\n",
    "            np.arange,\n",
    "            lambda _: np.divide(_, np.max(_)),\n",
    "            lambda _: np.multiply(_, np.ptp(arr)),\n",
    "            lambda _: np.add(_, np.min(arr))\n",
    "        )\n",
    "        digitized = np.digitize(arr, bins)\n",
    "        for i in np.unique(digitized):\n",
    "\n",
    "            cum_weight.append((subj.id, i, arr[digitized == i].sum()))\n",
    "    return pd.DataFrame(\n",
    "        cum_weight,\n",
    "        columns=[\"subject\", \"bin\", \"weight\"]\n",
    "    ).set_index([\"subject\", \"bin\"])\n",
    "cum_weight = edge_mass_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([\n",
    "    get_subj_metadata().to_xarray(),\n",
    "    cum_weight.to_xarray()\n",
    "])\n",
    "\n",
    "error_line(\n",
    "    __.pipe(\n",
    "        dict(\n",
    "            weight=ds.mean(\"subject\")[\"weight\"],\n",
    "            std=ds.std(\"subject\")[\"weight\"],\n",
    "        ),\n",
    "        lambda _: xr.merge([_]).to_dataframe().reset_index()\n",
    "    ),\n",
    "    x=\"bin\",\n",
    "    y=\"weight\",\n",
    "    # color=\"group\",\n",
    "    err=\"std\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Subjects\n",
    "\n",
    "Loop through all subjects and gather various metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def subject_graphs(filter_level = 1, drop = [], subject = None):\n",
    "    _sub = {\"subject\": subject} if subject is not None else {}\n",
    "    for bidsfile in layout_get(desc=[\"sift2\", \"avgFA\", \"medR1\"], **_sub):\n",
    "        if not (subj := Subject.from_bids_entry(bidsfile, filter_level)):\n",
    "            print(f\"Dropped {bidsfile.entities['subject']} (Not found in metadata)\")\n",
    "            continue\n",
    "\n",
    "        yield subj\n",
    "            \n",
    "\n",
    "\n",
    "def subject_properties(subj: Subject):\n",
    "        try:\n",
    "            G = subj.adj.graph\n",
    "            largest_connected_comp = max(nx.connected_components(G), key=len)\n",
    "            return {\n",
    "                \"subject\": subj.id,\n",
    "                \"category\": subj.group,\n",
    "                \"weight\": subj.weight,\n",
    "                \"degree\":np.mean([*zip(*G.degree(weight=\"weight\"))][1]),\n",
    "                \"num_regions\": len(G.nodes),\n",
    "                # \"dropped_regions\": list(drop_regions),\n",
    "                # \"num_connected_comps\": nx.number_connected_components(G),\n",
    "                # \"largest_connected_comp\": len(largest_connected_comp),\n",
    "                # \"density\": nx.density(G),\n",
    "                # \"transitivity\": nx.transitivity(G),\n",
    "                \"global_efficiency\": global_efficiency(G, weight=\"distance\"),\n",
    "                # \"local_efficiency\": local_efficiency(G, weight=\"distance\"),\n",
    "                # \"shortest_path\": nx.average_shortest_path_length(G.subgraph(largest_connected_comp), weight=\"distance\"),\n",
    "            }\n",
    "        except KeyError as err:\n",
    "            print(subj)\n",
    "            raise err\n",
    "\n",
    "\n",
    "def subject_df(drop_regions = []):\n",
    "    rows = []\n",
    "    with mp.Pool() as pool:\n",
    "        graphs = list(subject_graphs(drop=drop_regions))\n",
    "        rows = pool.map(subject_properties, graphs)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def xproperty_rank(ds, columns=[], inverse_columns=[]):\n",
    "    from scipy.stats import rankdata\n",
    "    def rank(ds, column, inverse=False):\n",
    "        data = ds[column].data\n",
    "        ranked = rankdata(data, axis=-1) / data.shape[-1]\n",
    "        return ds.assign({column+\"_rank\": (ds.dims, ranked)})\n",
    "    for column in columns:\n",
    "        ds = rank(ds, column)\n",
    "    for column in inverse_columns:\n",
    "        ds = rank(ds, column, inverse=True)\n",
    "    return ds\n",
    "\n",
    "def xhubness(ds):\n",
    "    cols = [\"betweenness\", \"degree\"]\n",
    "    inv_cols = [\"path_length\", \"clust_coeff\"]\n",
    "    return (\n",
    "        xproperty_rank(ds, columns=cols, inverse_columns=inv_cols)\n",
    "        .assign(\n",
    "            hubness = lambda ds: (\n",
    "                ds.dims,\n",
    "                __.pipe(\n",
    "                    cols + inv_cols,\n",
    "                    __.map(lambda s: s+\"_rank\"),\n",
    "                    __.map(ds.get),\n",
    "                    list,\n",
    "                    lambda _: np.sum(_, axis=0)\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "@nb_cache(\"data\")\n",
    "def get_data(thresholds = range(10)):\n",
    "    return __.pipe(\n",
    "        (\n",
    "            (threshold, subject_graphs(filter_level=threshold))\n",
    "            for threshold in thresholds\n",
    "        ),\n",
    "        __.starmap(lambda threshold, graphs: (\n",
    "            pd.DataFrame(\n",
    "                graph.adj.masked,\n",
    "                index=[\n",
    "                    pd.Index([graph.id], name=\"subject\").repeat(\n",
    "                        len(index := graph.adj.metadata.index)\n",
    "                    ),\n",
    "                    pd.Index([graph.weight], name=\"weight\").repeat(len(index)),\n",
    "                    pd.Index([threshold], name=\"threshold\").repeat(len(index)),\n",
    "                    index.rename(\"src\"),\n",
    "                ],\n",
    "                columns=index.rename(\"dest\")\n",
    "            ) for graph in graphs\n",
    "        )),\n",
    "        itx.flatten,\n",
    "        pd.concat,\n",
    "        lambda df: (\n",
    "            df\n",
    "            .stack()\n",
    "            .to_xarray()\n",
    "        ),\n",
    "        lambda da: xr.merge(\n",
    "            [\n",
    "                dict(\n",
    "                    adj=da,\n",
    "                    group=get_subj_metadata()['group'],\n",
    "                )\n",
    "            ],\n",
    "            join=\"inner\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def col_row_insert(matrix, pos, value):\n",
    "    return np.insert(np.insert(matrix, pos, value, axis=0), pos, value, axis=1)\n",
    "\n",
    "def _get_params(avg, weight, group, threshold):\n",
    "    adj = (\n",
    "        AdjacencyMatrix(\n",
    "            raw=avg\n",
    "                .sel(weight=weight, group=group)\n",
    "                .squeeze()\n",
    "                .fillna(0)\n",
    "                .pipe(lambda _: col_row_insert(_, 0, 0))\n",
    "                .data,\n",
    "            metadata=read_metadata(),\n",
    "        )\n",
    "        .mask_diagonal()\n",
    "        .mask_equal(0)\n",
    "        .mask_where(filter_logile(threshold))\n",
    "        # .mask_where_meta(MetadataMasks.src_and_dest(\"hemisphere\").equals(\"L\"))\n",
    "    )\n",
    "    adj.props[\"distance\"] = np.ma.filled(1/adj.raw, np.NaN)\n",
    "    return graph_params(\n",
    "        adj, weight=str(weight.data), group=str(group.data), threshold=threshold\n",
    "    )\n",
    "\n",
    "@nb_cache('group_avg_params')\n",
    "def group_avg_params():\n",
    "    avg = get_data().sel(threshold=0).groupby(\"group\").mean().to_array()\n",
    "    \n",
    "    with mp.Pool() as pool:\n",
    "        return pd.concat(\n",
    "            pool.starmap(\n",
    "                _get_params,\n",
    "                it.product(avg, avg['weight'], avg['group'], range(10))\n",
    "            )\n",
    "        )\n",
    "    # dropped = drop(sub) if callable(drop) else drop\n",
    "    # df.drop(index=dropped, columns=dropped, inplace=True)\n",
    "    \n",
    "def nodal_properties():\n",
    "    return pd.read_csv('results/nodal_props.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Subject wide summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@nb_cache(\"subject_full_df\")\n",
    "def subject_full_df():\n",
    "    return subject_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = subject_full_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: global-props\n",
    "#| fig-cap: |\n",
    "#|   Global network properties compared across study groups. The top row shows the\n",
    "#|   average network-wide degree, and the bottom row shows the average global\n",
    "#|   efficiency. In the left column, networks are weighted by sift2 corrected streamline\n",
    "#|   count, and in the right column, by average FA along the fiber.\n",
    "def subject_distributions(col, weight):\n",
    "    fig = px.box(\n",
    "        df[(df[\"weight\"] == weight) & (df[\"subject\"] != 43)],\n",
    "        x=\"category\",\n",
    "        color=\"category\",\n",
    "        y=col,\n",
    "        points=\"outliers\",\n",
    "        width=584,\n",
    "        height=400,\n",
    "        labels={\n",
    "            \"num_regions\": \"# Regions\",\n",
    "            \"category\": \"Group\"\n",
    "        },\n",
    "        title=titleize(col),\n",
    "        hover_data=[\"subject\"]\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=50, r=50, t=50, b=50),\n",
    "        showlegend=False\n",
    "    )\n",
    "    return fig\n",
    "cols = [\n",
    "    # \"transitivity\",\n",
    "    # \"efficiency\",\n",
    "    # \"density\",\n",
    "    # \"num_connected_comps\",\n",
    "    # \"largest_connected_comp\",\n",
    "    \"degree\",\n",
    "    \"global_efficiency\"\n",
    "]\n",
    "# plotly_tabulate(lambda _: subject_distributions(_, \"sift2\"), cols)\n",
    "fig = plotly_grid(subject_distributions, cols, [\"sift2\", \"avgFA\"], vspacing=0.15)\n",
    "# fig = subject_distributions(\"degree\", \"avgFA\")\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    # title_text=\"Global Graph properties\",\n",
    "    title = dict(\n",
    "        # text=f'<b>{titleize(fig.layout[\"title\"][\"text\"])}</b>',\n",
    "        text=\"\",\n",
    "        font_family=\"roboto\",\n",
    "        font_color=\"#58595b\",\n",
    "        font_size=18,\n",
    "        xanchor=\"center\",\n",
    "        x = 0.5,\n",
    "        xref=\"paper\",\n",
    "    ),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(\n",
    "        t=50,\n",
    "        b=50,\n",
    "        l=50,\n",
    "        r=50,\n",
    "    ),\n",
    "    template=\"seaborn\"\n",
    ")\n",
    "D.Image(fig.to_image(format='png'))\n",
    "\n",
    "\n",
    "# figures_to_html([\n",
    "#     subject_distributions(col)\n",
    "#     for col in [\n",
    "#         \"transitivity\",\n",
    "#         \"efficiency\",\n",
    "#         \"density\",\n",
    "#         \"num_connected_comps\",\n",
    "#         \"largest_connected_comp\",\n",
    "#         \"degree\",\n",
    "#     ]\n",
    "# ], \"pages/bn246/subject_distributions.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Subject wide distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_props = nodal_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"betweenness\",\n",
    "    \"clust_coeff\",\n",
    "    \"path_length\",\n",
    "    \"degree\",\n",
    "]\n",
    "indexed = property_rank(\n",
    "    node_props.set_index([\"weight\", \"category\", \"subject\", \"node\"]),\n",
    "    columns=cols\n",
    ")\n",
    "def hubness_property_rank_distribution(col, weight):\n",
    "    return distribution_plot(indexed.loc[weight], x=col+\"_rank\", y=col)\n",
    "# figures_to_html(\n",
    "#     [\n",
    "#         distribution_plot(indexed, x=col+\"_rank\", y=col) \n",
    "#         for col in cols\n",
    "#     ],\n",
    "#     filename=\"pages/bn246/nodal_distributions.html\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"betweenness\",\n",
    "    \"degree\",\n",
    "    \"clust_coeff\",\n",
    "    \"path_length\",\n",
    "]\n",
    "plotly_tabulate(lambda _: hubness_property_rank_distribution(_, \"sift2\"), cols)\n",
    "# fig = plotly_grid(hubness_property_rank_distribution, cols, [\"sift2\", \"avgFA\"])\n",
    "# fig.update_layout(\n",
    "#     showlegend=False,\n",
    "#     title_text=\"Node Ranking Distributions\",\n",
    "#     title_xanchor=\"left\",\n",
    "#     title_x = 0,\n",
    "#     title_xref=\"paper\",\n",
    "#     height=800,\n",
    "#     width=1000,\n",
    "#     template=\"seaborn\"\n",
    "# )\n",
    "# D.Image(fig.to_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hub_disruption_index(group, cols):\n",
    "    df = nodal_properties()\n",
    "    indexed = df.set_index([\"category\"])\n",
    "    hc = indexed.loc[\"HC\"]\n",
    "    avg = hc.groupby(\"node\").mean()[cols]\n",
    "    group_values = indexed.loc[group].reset_index().pivot(columns=\"node\", index=\"subject\", values=cols)\n",
    "    delta = group_values - avg\n",
    "    delta = pd.DataFrame(delta.stack(), columns=[\"delta\"])\n",
    "    delta[\"avg\"] = avg.reindex(delta.index, level=\"node\")\n",
    "    delta[\"Delta Total\"] = delta.reset_index().groupby(\"subject\").sum()[\"delta\"].reindex(delta.index, level=\"subject\")\n",
    "    return delta.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= [\n",
    "    \"betweenness\",\n",
    "    # \"clust_coeff\",\n",
    "    # \"path_length\",\n",
    "    # \"degree\",\n",
    "]\n",
    "plotly_tabulate(\n",
    "    lambda group: plotly_tabulate(\n",
    "        px.scatter(\n",
    "            hub_disruption_index(group, col),\n",
    "            x=\"avg\",\n",
    "            y=\"delta\",\n",
    "            color=\"Delta Total\",\n",
    "            trendline=\"ols\",\n",
    "            title=titleize(col),\n",
    "            hover_data=[\"subject\", \"node\"]\n",
    "        )\n",
    "        for col in cols\n",
    "    ),\n",
    "    cats.values()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hubness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hubs():\n",
    "    return xhubness(\n",
    "        nodal_properties()\n",
    "        .set_index(['weight', 'threshold', 'subject', 'node'])\n",
    "        .to_xarray()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_avg_hubs():\n",
    "    return xhubness(\n",
    "        group_avg_params()\n",
    "        .set_index(['weight', 'group', 'threshold', 'node'])\n",
    "        .to_xarray()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "mds = metadata_ds()\n",
    "hub_data = (\n",
    "    mds\n",
    "    .merge(get_hubs())\n",
    "    .assign(\n",
    "        node_uid=lambda ds: ds[\"Name\"] + \"_\" + ds[\"hemisphere\"],\n",
    "        avg_hubness = lambda ds: ds[\"hubness\"].mean(dim=\"subject\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "avg_hub_data = (\n",
    "    mds\n",
    "    .drop_dims(\"subject\")\n",
    "    .merge(get_avg_hubs())\n",
    "    .assign(node_uid=lambda ds: ds[\"Name\"] + \"_\" + ds['hemisphere'])\n",
    ")\n",
    "\n",
    "def get_labels(ds, fields, repeats):\n",
    "    labels = (\n",
    "        np.repeat(hub_data[fields].to_pandas().to_numpy(), repeats, axis=0)\n",
    "        .reshape((-1, repeats, len(fields)))\n",
    "    )\n",
    "    template = \"<br>\".join(f\"<b>{name}:</b> %{{customdata[{i}]}}\" for i, name in enumerate(fields))\n",
    "    return labels, template\n",
    "# cols = [\"degree\", \"clust_coeff\", \"path_length\", \"betweenness\"]\n",
    "import plotly.express as px\n",
    "\n",
    "labels, template = get_labels(hub_data, [\"Long Name\", \"Gyrus\", \"Lobe\", \"hemisphere\"], len(hub_data[\"subject\"]))\n",
    "\n",
    "\n",
    "# fig.data[0][\"hovertemplate\"] += \"<br>\" + template\n",
    "# fig.update_traces(\n",
    "#     customdata=labels,\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a composite hubness score using 4 individual measures of hubness: degree, betweenness, clustering coefficient, and shortest path length. Each node in each individual subject was rank-ordered with respect to each of the four metrics and assigned a score between 0 and 1 corresponding to the rank. These scores were summed together to get a hubness score between 0 and 4. Rank-ordering the nodes of each subject by hubness score gives the hubness rank for each subject. The hubness ranking varied little across the subjects (figure TBD).\n",
    "\n",
    "The hubness scores from each subject were averaged to get the group hubness scores for each node. The group-wise hubness rankings are compared below.\n",
    "\n",
    "Note that if, instead of the subject-wise analysis described above, the subject connectomes are first combined into group-consensus connectivity matrices, a large amount of difference manifests between groups, especially when FA is used as the weight (figure TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: hubness-ranking\n",
    "#| fig-cap: |\n",
    "#|   Average hubness scores across group members of brainnetome atlas nodes. Nodes are\n",
    "#|   sorted according to increasing hubness in healthy controls. Nodes in disease groups\n",
    "#|   are shown in the same order for comparison.\n",
    "def template_metadata(ds, fields):\n",
    "    labels = np.dstack([ds[field] for field in fields] or [[]])\n",
    "    template = \"<br>\".join(\n",
    "        f\"<b>{name}:</b> %{{customdata[{i}]}}\" for i, name in enumerate(fields)\n",
    "    )\n",
    "    return labels, template\n",
    "\n",
    "def hubness_score(weight, sort_weight=None):\n",
    "    hubness_col = \"hubness\"\n",
    "    get_ds = lambda _weight: (\n",
    "        hub_data\n",
    "        .swap_dims({\"node\": \"node_uid\"})\n",
    "        .groupby(\"group\")\n",
    "        .mean()\n",
    "        .sel(weight=_weight, threshold=abs0)\n",
    "        .reindex(group=[\"HC\", \"FEP\", \"High risk\", \"Treatment 3+ yr\"])\n",
    "    )\n",
    "    _ds = get_ds(weight)\n",
    "    if sort_weight is None or sort_weight == weight:\n",
    "        fig = px.imshow(\n",
    "            _ds.sortby(_ds.sel(group=\"HC\")[hubness_col])[hubness_col],\n",
    "            aspect='auto',\n",
    "            color_continuous_scale=itx.nth(zip(*spectral), 1),\n",
    "        )\n",
    "    else:\n",
    "        _sort_ds = get_ds(sort_weight)\n",
    "        fig = px.imshow(\n",
    "            _ds.sortby(_sort_ds.sel(group=\"HC\")[hubness_col])[hubness_col],\n",
    "            aspect=\"auto\",\n",
    "            color_continuous_scale=itx.nth(zip(*spectral), 1),\n",
    "        )\n",
    "\n",
    "    labels, template = template_metadata(hub_data, [\"hemisphere\", \"Lobe\", \"Gyrus\", \"Long Name\"])\n",
    "    fig.data[0].customdata = labels\n",
    "    fig.data[0].hovertemplate += \"<br>\" + template\n",
    "    return fig\n",
    "\n",
    "# fig.update_xaxes(scaleanchor=\"y\", scaleratio=1, type=\"category\")\n",
    "fig = plotly_grid(\n",
    "    lambda _: hubness_score(_),\n",
    "    [\"sift2\", \"avgFA\"],\n",
    "    vspacing=0.35\n",
    ")\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    # coloraxis_showscale=False,\n",
    "    title=dict(\n",
    "        text=\"Hubness Scores\",\n",
    "        xanchor=\"left\",\n",
    "        x = 0,\n",
    "        xref=\"paper\",\n",
    "    ),\n",
    "    height=600,\n",
    "    width=1500,\n",
    "    margin_r=20,\n",
    "    # paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    ")\n",
    "# fig.update_xaxes(\n",
    "#     tickfont_size=5,\n",
    "# )\n",
    "D.Image(fig.to_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting\n",
    "def show_atlas(hem):\n",
    "    infl_left = itx.one(layout.get(subject=\"001\", suffix=\"inflated\", hemi=hem))\n",
    "    dparc = nib.load(\n",
    "        itx.one(layout.get(subject=\"001\", suffix=\"dparc\", atlas=\"bn210\", hemi=hem))\n",
    "    )\n",
    "    data = dparc.get_arrays_from_intent('NIFTI_INTENT_LABEL')[0]\n",
    "    ds = hub_data.sel(weight=\"sift2\").groupby(\"group\").mean()\n",
    "    h = xr.concat(\n",
    "        [\n",
    "            xr.DataArray([0], dims=(\"node\",), coords={\"node\": [0]}),\n",
    "            ds.sel(threshold=3, group=\"HC\")['hubness'],\n",
    "        ],\n",
    "        dim=\"node\"\n",
    "    )\n",
    "    lookup = h.loc[data.data]\n",
    "    fsaverage = datasets.fetch_surf_fsaverage()\n",
    "    return plotting.view_surf(\n",
    "        infl_left.path,\n",
    "        lookup.data,\n",
    "        cmap='nipy_spectral',\n",
    "        symmetric_cmap=False,\n",
    "        # colorbar=False,\n",
    "    )\n",
    "\n",
    "show_atlas(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hubs=(\n",
    "    hub_data\n",
    "    .sel(weight=\"sift2\")\n",
    "    .groupby(\"group\")\n",
    "    .mean()[\"hubness\"]\n",
    "    .sel(group=\"FEP\")\n",
    "    .squeeze()\n",
    "    .reset_coords(drop=True)\n",
    ")\n",
    "avg_hubs = (\n",
    "    avg_hub_data\n",
    "    .sel(weight=\"sift2\")\n",
    "    [\"hubness\"]\n",
    "    .squeeze()\n",
    "    .reset_coords(drop=True)\n",
    ")\n",
    "data = get_data().drop_sel(subject=80)\n",
    "nbs_arr = xr.concat(\n",
    "    [\n",
    "        xr.DataArray(\n",
    "            scipy.io.loadmat(\"results/nbs/sift2_nbs.mat\")[\"test_stat\"],\n",
    "            coords={\n",
    "                \"src\": mds[\"node\"].data,\n",
    "                \"dest\": mds[\"node\"].data,\n",
    "            }\n",
    "        ).expand_dims(weight=[\"sift2\"]),\n",
    "        xr.DataArray(\n",
    "            scipy.io.loadmat(\"results/nbs/nbs.mat\")[\"test_stat\"],\n",
    "            coords={\n",
    "                \"src\": mds[\"node\"].data,\n",
    "                \"dest\": mds[\"node\"].data,\n",
    "            }\n",
    "        ).expand_dims(weight=['avgFA']),\n",
    "    ],\n",
    "    dim=\"weight\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEIGHT = \"avgFA\"\n",
    "ctrl = data.mean(dim='subject').sel(weight=WEIGHT, drop=True)[\"adj\"]\n",
    "diff = data.assign(\n",
    "    diff=data[\"adj\"] - ctrl,\n",
    "    nbs=nbs_arr\n",
    ")\n",
    "\n",
    "# def dim_map(func, ds, dims=None):\n",
    "#     for \n",
    "\n",
    "def edge_hubness(hubs):\n",
    "    return __.pipe(\n",
    "        hubs['threshold'],\n",
    "        __.map(lambda thresh: __.pipe(\n",
    "            hubs['subject'],\n",
    "            __.map(lambda subject: __.pipe(\n",
    "                hubs.sel(threshold=thresh, subject=subject).to_numpy()[None, ...],\n",
    "                lambda _: np.repeat(_, 2, axis=0),\n",
    "                lambda _: np.add.outer(*_),\n",
    "                lambda _: (\n",
    "                    xr.DataArray(\n",
    "                        _,\n",
    "                        dims=(\"src\", \"dest\"),\n",
    "                        coords={\"src\": hubs[\"node\"].data, \"dest\": hubs[\"node\"].data}\n",
    "                    )\n",
    "                    .expand_dims({\"threshold\": [thresh.data], \"subject\": [subject.data]})\n",
    "                ),\n",
    "            )),\n",
    "            lambda _: xr.concat(_, dim=\"subject\")\n",
    "        )),\n",
    "        lambda _: xr.concat(_, dim=\"threshold\")\n",
    "    )\n",
    "\n",
    "def edge_hub_diff(hubs):\n",
    "    return __.pipe(\n",
    "        hubs['threshold'],\n",
    "        __.map(lambda thresh: __.pipe(\n",
    "            hubs['subject'],\n",
    "            __.map(lambda subject: __.pipe(\n",
    "                hubs.sel(threshold=thresh, subject=subject).to_numpy()[None, ...],\n",
    "                lambda _: np.repeat(_, 2, axis=0),\n",
    "                lambda _: np.subtract.outer(*_),\n",
    "                np.abs,\n",
    "                lambda _: (\n",
    "                    xr.DataArray(\n",
    "                        _,\n",
    "                        dims=(\"src\", \"dest\"),\n",
    "                        coords={\"src\": hubs[\"node\"].data, \"dest\": hubs[\"node\"].data}\n",
    "                    )\n",
    "                    .expand_dims({\"threshold\": [thresh.data], \"subject\": [subject.data]})\n",
    "                ),\n",
    "            )),\n",
    "            lambda _: xr.concat(_, dim=\"subject\")\n",
    "        )),\n",
    "        lambda _: xr.concat(_, dim=\"threshold\")\n",
    "    )\n",
    "\n",
    "def mean_top(var: str, number: int, dim, *args, **kwargs):\n",
    "    def inner(ds):\n",
    "        dim_order = ds[var].dims\n",
    "        axis = ds[var].get_axis_num(dim)\n",
    "        sel = ds[var].fillna(0).argsort(axis=axis).isel({dim: slice(-number, None)})\n",
    "        return __.pipe(\n",
    "            ds,\n",
    "            # __.filter(lambda _: _ == \"diff\"),\n",
    "            __.filter(lambda _: set(ds[_].dims) == set(dim_order)),\n",
    "            __.map(lambda _var: (_var, ds[_var].transpose(*dim_order))),\n",
    "            __.starmap(lambda name, da: xr.DataArray(\n",
    "                np.nanmean(np.take_along_axis(da.values, sel, axis=axis), axis=axis),\n",
    "                coords = __.pipe(\n",
    "                    da.coords.items(),\n",
    "                    __.filter(lambda _: _[0] != dim),\n",
    "                    dict,\n",
    "                ),\n",
    "                dims = __.pipe(\n",
    "                    da.dims,\n",
    "                    __.filter(lambda _: _ != dim),\n",
    "                    tuple\n",
    "                ),\n",
    "                name=name,\n",
    "            )),\n",
    "            xr.merge,\n",
    "        )\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def quiet_std(x):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return x.std(\"subject\")\n",
    "\n",
    "\n",
    "def edge_distribution_prefilter():\n",
    "    return (\n",
    "        data\n",
    "        .groupby(\"group\")\n",
    "        .std()\n",
    "        .mean(dim=\"src\")\n",
    "        .rename({\"dest\": \"node\", \"adj\": \"std_prefilter\"})\n",
    "    )\n",
    "\n",
    "def edge_distribution():\n",
    "    std =  (\n",
    "        diff\n",
    "        .groupby(\"group\")\n",
    "        .median()\n",
    "        ['adj']\n",
    "        .std(dim='group')\n",
    "    )\n",
    "    mean = (\n",
    "        diff\n",
    "        .groupby(\"group\")\n",
    "        .median()\n",
    "        ['adj']\n",
    "        .mean(dim=\"group\")\n",
    "    )\n",
    "    return (\n",
    "        (std / mean)\n",
    "        .mean(dim='src')\n",
    "        .rename({\"dest\": \"node\"})#, \"adj\": \"std\"})\n",
    "        .rename('std')\n",
    "        \n",
    "    )\n",
    "\n",
    "def tstat():\n",
    "    from scipy.stats import ttest_ind\n",
    "    grouped = diff.groupby('group')\n",
    "    hc = grouped['HC']['adj'].data\n",
    "    fep = grouped['FEP']['adj'].data\n",
    "    sub_dim = diff['adj'].dims.index('subject')\n",
    "    t, p = ttest_ind(hc, fep, axis=sub_dim, nan_policy='omit', alternative='two-sided')\n",
    "    red = diff.mean('subject')\n",
    "    return xr.DataArray(t, dims=red.dims, coords=red.coords, name='tstat').mean('src').rename(dest='node')\n",
    "    \n",
    "\n",
    "def nbs_probability():\n",
    "    return (\n",
    "        diff\n",
    "        ['nbs']\n",
    "        .pipe(lambda ds: ds > 3)\n",
    "        .mean(dim='src')\n",
    "        .rename({\"dest\": \"node\"})#, \"adj\": \"std\"})\n",
    "        .rename('nbs_prob')\n",
    "        \n",
    "    )\n",
    "\n",
    "def abs_deviation():\n",
    "    return (\n",
    "        diff\n",
    "        .groupby(\"group\")\n",
    "        .mean()\n",
    "        ['diff']\n",
    "        .pipe(abs)\n",
    "        .mean(dim=\"src\")\n",
    "        .rename(dest=\"node\")\n",
    "        .rename(\"abs_diff\")\n",
    "    )\n",
    "\n",
    "def positive_deviation():\n",
    "    return (\n",
    "        diff\n",
    "        .groupby(\"group\")\n",
    "        .mean()\n",
    "        ['diff']\n",
    "        .pipe(lambda _: np.maximum(_, 0))\n",
    "        .where(lambda ds: ds!= 0)\n",
    "        .mean(dim=\"src\")\n",
    "        .pipe(abs)\n",
    "        .rename(dest=\"node\")\n",
    "        .rename(\"pos_diff\")\n",
    "    )\n",
    "\n",
    "def negative_deviation():\n",
    "    return (\n",
    "        diff\n",
    "        .groupby(\"group\")\n",
    "        .mean()\n",
    "        ['diff']\n",
    "        .pipe(lambda _: np.minimum(_, 0))\n",
    "        .where(lambda ds: ds!= 0)\n",
    "        .mean(dim=\"src\")\n",
    "        .pipe(abs)\n",
    "        .rename(dest=\"node\")\n",
    "        .rename(\"neg_diff\")\n",
    "    )\n",
    "\n",
    "def node_wise_std():\n",
    "    return (\n",
    "        data\n",
    "        .groupby('group')\n",
    "        .std()\n",
    "        ['adj']\n",
    "        .mean(dim='src')\n",
    "        .rename(dest=\"node\")\n",
    "        .rename(\"node_wise_std\")\n",
    "    )\n",
    "\n",
    "def negative_deviation_prefilter():\n",
    "    return (\n",
    "        diff\n",
    "        .assign(diff=lambda ds: np.minimum(ds['diff'], 0).where(lambda ds: ds!=0))\n",
    "        .groupby(\"group\")\n",
    "        .mean()\n",
    "        ['diff']\n",
    "        # .where(lambda ds: ds!=0)\n",
    "        .mean(dim=\"src\")\n",
    "        .pipe(abs)\n",
    "        .rename(dest=\"node\")\n",
    "        .rename(\"neg_diff_prefilter\")\n",
    "    )\n",
    "\n",
    "def edge_deviation(hub_weight='sift2'):\n",
    "    hubs = (\n",
    "        hub_data\n",
    "        .sel(weight=hub_weight)\n",
    "        [\"hubness\"]\n",
    "    )\n",
    "    return (\n",
    "        diff\n",
    "        .assign(\n",
    "            edge_hubness=edge_hubness(hubs),\n",
    "            edge_hub_diff=edge_hub_diff(hubs),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "def mean_edge_deviation(hub_weight=\"sift2\"):\n",
    "    hubs = (\n",
    "        hub_data\n",
    "        .sel(weight=hub_weight)\n",
    "        [\"hubness\"]\n",
    "    )\n",
    "    map_to_dest = lambda ds: ds.rename({\"node\": \"src\"}).expand_dims(dest=diff[\"dest\"])\n",
    "    return (\n",
    "        diff\n",
    "        .assign(\n",
    "            edge_hubness=edge_hubness(hubs),\n",
    "            dest_hubness=hubs.pipe(map_to_dest),\n",
    "            dest_id=hubs[\"node\"].pipe(map_to_dest),\n",
    "        )\n",
    "        .groupby(\"group\")\n",
    "        .mean(dim=['subject', 'src'])\n",
    "        # .pipe(mean_top(\"diff\", 100, dim=\"src\"))\n",
    "        .rename({\"dest\": \"node\"})\n",
    "        .assign(\n",
    "            src_hubness=hub_data\n",
    "                .sel(weight=hub_weight)\n",
    "                .groupby(\"group\")\n",
    "                .mean()\n",
    "                [\"hubness\"],\n",
    "            degree_rank=hub_data\n",
    "                .sel(weight=hub_weight)\n",
    "                .groupby(\"group\")\n",
    "                .mean()\n",
    "                [\"degree_rank\"]\n",
    "        )\n",
    "        .assign(edge_check=lambda ds: ds[\"src_hubness\"] + ds[\"dest_hubness\"])\n",
    "        .merge(edge_distribution())\n",
    "        # .merge(negative_deviation())\n",
    "        # .merge(negative_deviation_prefilter())\n",
    "        # .merge(edge_distribution_prefilter())\n",
    "        # .merge(abs_deviation())\n",
    "        .merge(node_wise_std())\n",
    "        .merge(tstat())\n",
    "        .merge(nbs_probability())\n",
    "        # .sel(group=\"FEP\", threshold=5)\n",
    "    )\n",
    "f = mean_edge_deviation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.violin(\n",
    "    f.merge(mds.drop_dims('subject')).sel(weight='avgFA', threshold=0, group='FEP')[['hemisphere', 'neg_diff_prefilter']].to_dataframe().reset_index(),\n",
    "    x='hemisphere',\n",
    "    y='neg_diff_prefilter'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    edge_deviation()\n",
    "    .groupby('group')\n",
    "    .mean('subject')\n",
    "    .mean('group')\n",
    "    .sel(threshold=0, weight='avgFA')\n",
    "    .to_dataframe(),\n",
    "    \n",
    "    x='edge_hubness',\n",
    "    y='edge_hub_diff',\n",
    "    height=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    edge_deviation()\n",
    "    .groupby('group')\n",
    "    .mean('subject')\n",
    "    .assign(adj_std=lambda ds: ds['adj'].std('group'))\n",
    "    .mean('group')\n",
    "    .sel(threshold=0, weight='avgFA')\n",
    "    .to_dataframe()\n",
    ")    \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "X = df[[\"edge_hubness\", \"edge_hub_diff\"]]\n",
    "y = df[\"adj\"]\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    edge_deviation()\n",
    "    .sel(threshold=2, weight='avgFA')\n",
    "    .groupby('group')\n",
    "    .mean('subject')\n",
    "    .assign(adj_std=lambda ds: ds['adj'].std('group') / ds['adj'].mean('group'))\n",
    "    .mean('group')\n",
    "    # .assign(adj=lambda ds: np.log(ds['adj']))\n",
    "    .to_dataframe(),\n",
    "    \n",
    "    x='edge_hubness',\n",
    "    y='edge_hub_diff',\n",
    "    z='adj_std',\n",
    "    color=\"adj\",\n",
    "    height=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: hubness-deviation-corr\n",
    "#| fig-cap: |\n",
    "#|   Correlation between hubness and mean negative deviation. Each row shows the\n",
    "#|   analysis performed at a different graph threshold. Edge deviations are calculated\n",
    "#|   relative to the HC mean weight, using average FA as the weight. Hubness scores are\n",
    "#|   calculated using sift2-corrected streamline count as the weight. FEP deviations are\n",
    "#|   shown in red; HC deviations are shown in blue. The R^2^ value for FEP deviations\n",
    "#|   are shown above each graph.\n",
    "def max_edge_deviation_fig(thresh):\n",
    "    fig = px.scatter(\n",
    "        f.sel(weight=\"avgFA\", threshold=thresh, group=[\"HC\"]).to_dataframe().reset_index(),\n",
    "        x=\"src_hubness\",\n",
    "        y=\"std\",\n",
    "        color=\"group\",\n",
    "        trendline=\"ols\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        # xaxis_title_text=\"Max Edge deviation\",\n",
    "        # title_text=\"Max Elevation\",\n",
    "        title_text=\"\",\n",
    "        # width=1000,\n",
    "    )\n",
    "    r_val = px.get_trendline_results(fig)[lambda df: df['group'] == 'HC'].iloc[0][\"px_fit_results\"].rsquared\n",
    "    fig.add_annotation(\n",
    "        x=0.05,\n",
    "        y=1.01,\n",
    "        xref=\"x domain\",\n",
    "        yref=\"y domain\",\n",
    "        text=f\"R<sup>2</sup> = {r_val:.5f}\",\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"bottom\",\n",
    "        # xref=\"paper\",\n",
    "        # yref=\"paper\",\n",
    "        showarrow=False,\n",
    "        font_size=14,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plotly_grid(max_edge_deviation_fig, [0], vspacing=0.05)\n",
    "# fig = max_edge_deviation_fig(4, \"FEP\")\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=1200,\n",
    ")\n",
    "D.Image(fig.to_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting\n",
    "def show_atlas(hem):\n",
    "    infl_left = itx.one(layout.get(subject=\"001\", suffix=\"inflated\", hemi=hem))\n",
    "    dparc = nib.load(\n",
    "        itx.one(layout.get(subject=\"001\", suffix=\"dparc\", atlas=\"bn210\", hemi=hem))\n",
    "    )\n",
    "    data = dparc.get_arrays_from_intent('NIFTI_INTENT_LABEL')[0]\n",
    "    h = xr.concat(\n",
    "        [\n",
    "            xr.DataArray([0], dims=(\"node\",), coords={\"node\": [0]}),\n",
    "            f.sel(weight='avgFA', threshold=3, group=\"FEP\")[\"std\"],\n",
    "        ],\n",
    "        dim=\"node\"\n",
    "    )\n",
    "    lookup = h.loc[data.data]\n",
    "    fsaverage = datasets.fetch_surf_fsaverage()\n",
    "    return plotting.view_surf(\n",
    "        infl_left.path,\n",
    "        lookup.data,\n",
    "        cmap='nipy_spectral',\n",
    "        symmetric_cmap=False,\n",
    "        # colorbar=False,\n",
    "    )\n",
    "show_atlas(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_data = data.groupby(\"group\").mean(dim=\"subject\")\n",
    "avg_diff = avg_data.assign(diff=(avg_data[\"adj\"] - ctrl) / ctrl)\n",
    "rng = np.random.default_rng(seed=2)\n",
    "\n",
    "\n",
    "def get_window(args):\n",
    "    diff, hubs, group, hubness, threshold = args\n",
    "    ds = diff.sel(\n",
    "        threshold=threshold,\n",
    "        weight=\"avgFA\",\n",
    "        group=group,\n",
    "    )\n",
    "    nodes = hubs.sel(group=group, threshold=threshold).where(\n",
    "        lambda ds: ds > hubness, drop=True\n",
    "    )\n",
    "    main = get_deviation(ds, nodes[\"node\"].data)\n",
    "    randoms = np.fromfunction(\n",
    "        np.vectorize(lambda _: get_random_deviations(ds, len(nodes[\"node\"]))),\n",
    "        shape=tuple([1000]),\n",
    "    )\n",
    "    mean = randoms.mean()\n",
    "    std = randoms.std()\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"group\": group.item(),\n",
    "                \"hubness\": hubness.item(),\n",
    "                \"threshold\": threshold,\n",
    "                \"deviation\": main.data,\n",
    "                \"rand_deviation\": mean,\n",
    "                \"rand_deviation_std\": std,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_deviation(ds, nodes):\n",
    "    return (\n",
    "        ds.sel(dest=nodes, src=nodes)[\"diff\"]\n",
    "        .pipe(lambda _: np.minimum(_, 0))\n",
    "        .where(lambda ds: ds != 0)\n",
    "        .pipe(abs)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_random_deviations(ds, num):\n",
    "    nodes = rng.choice(avg_hubs[\"node\"], num, replace=False)\n",
    "    return get_deviation(ds, nodes)\n",
    "\n",
    "\n",
    "@nb_cache(\"windowed\")#, reset_cache=True)\n",
    "def get_windowed():\n",
    "    params = lambda: it.product(\n",
    "        [avg_diff],\n",
    "        [avg_hubs],\n",
    "        avg_hubs[\"group\"],\n",
    "        avg_hubs.sel(group=\"FEP\", threshold=threshold).pipe(np.sort)[::20],\n",
    "        range(9),\n",
    "    )\n",
    "    with mp.Pool() as pool:\n",
    "        threshold = 1\n",
    "        return pd.concat(\n",
    "            __.pipe(\n",
    "                pool.imap(\n",
    "                    get_window,\n",
    "                    params(),\n",
    "                ),\n",
    "                lambda _: tqdm.tqdm(_, total=itx.ilen(params())),\n",
    "                list,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "windowed = (\n",
    "    get_windowed()\n",
    "    .assign(deviation_std=0)\n",
    "    .set_index([\"group\", \"hubness\", \"threshold\"])\n",
    "    .to_xarray()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: hubness-window\n",
    "#| fig-cap: |\n",
    "#|   Deviation within hubness core. X-axis tracks the minimum hubness of the core being\n",
    "#|   considered. The blue line tracks the total negative deviation within the core, the\n",
    "#|   red line tracks the average negative deviation within random subnetworks of the\n",
    "#|   same size as the core. All deviations are shown as absolute negative deviations.\n",
    "_windowed = windowed.sel(group=\"FEP\")\n",
    "means = (\n",
    "    _windowed\n",
    "    .drop(['rand_deviation_std', 'deviation_std'])\n",
    "    .to_array(dim=\"var\", name=\"deviation\")\n",
    ")\n",
    "std = (\n",
    "    _windowed\n",
    "    .drop(['rand_deviation', 'deviation'])\n",
    "    .rename({'rand_deviation_std': 'rand_deviation', 'deviation_std': 'deviation'})\n",
    "    .to_array(dim='var', name=\"std\")\n",
    ")\n",
    "\n",
    "\n",
    "fig = error_line(\n",
    "    xr.merge([means, std]).sel(threshold=5).to_dataframe().reset_index(),\n",
    "    x=\"hubness\",\n",
    "    y=\"deviation\",\n",
    "    color=\"var\",\n",
    "    err=\"std\",\n",
    "    markers=True,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    # title_text=\"Global Graph properties\",\n",
    "    title = dict(\n",
    "        # text=f'<b>{titleize(fig.layout[\"title\"][\"text\"])}</b>',\n",
    "        text=\"\",\n",
    "        font_family=\"roboto\",\n",
    "        font_color=\"#58595b\",\n",
    "        font_size=18,\n",
    "        xanchor=\"center\",\n",
    "        x = 0.5,\n",
    "        xref=\"paper\",\n",
    "    ),\n",
    "    height=400,\n",
    "    width=800,\n",
    "    # margin=dict(\n",
    "    #     t=50,\n",
    "    #     b=50,\n",
    "    #     l=50,\n",
    "    #     r=50,\n",
    "    # ),\n",
    "    template=\"seaborn\"\n",
    ")\n",
    "D.Image(fig.to_image(format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "d = mean_edge_deviation()\n",
    "__.pipe(\n",
    "    # it.product(diff_by_hubs['threshold'], [\"HC\", \"FEP\"]),\n",
    "    diff.sel(subject=diff[\"group\"] == \"FEP\")[\"subject\"],\n",
    "    __.filter(lambda _: _!=80),\n",
    "    __.map(lambda subj : __.pipe(\n",
    "        d.sel(subject=subj, threshold=5)\n",
    "        .to_dataframe(),\n",
    "\n",
    "        lambda _: sm.OLS(_[\"src_hubness\"], sm.add_constant(_[\"diff\"]), missing='drop').fit(),\n",
    "        lambda _: pd.DataFrame(\n",
    "            [[*_.params, _.rsquared]],\n",
    "            columns=[\"const\", \"hubness\", \"rsquared\"],\n",
    "            # index=pd.Index([(group, int(thresh))], name=(\"group\", \"threshold\"))\n",
    "            index=pd.Index([subj], name=\"subject\")\n",
    "        )\n",
    "    )),\n",
    "    pd.concat,\n",
    "    # pd.DataFrame.sort_index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter(\n",
    "#     grouper\n",
    "#     .median()\n",
    "#     .assign(std=grouper.map(quiet_std)[\"diff\"])\n",
    "#     .stack({\"edge\": [\"src\", \"dest\"]})\n",
    "#     .sel(group=\"FEP\")\n",
    "#     .to_dataframe()\n",
    "#     ,\n",
    "#     x=\"diff\",\n",
    "#     y=\"hubness\",\n",
    "#     # trendline=\"ols\",\n",
    "#     color=\"std\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.merge([\n",
    "    pd.read_csv(\n",
    "        itx.one(layout_get(desc=\"avgFA\", suffix=\"richclub\", subject=\"001\")), index_col=0\n",
    "    )\n",
    "    .assign(subject=1)\n",
    "    .set_index([\"subject\", \"threshold\", \"k\"], drop=True)\n",
    "    .to_xarray(),\n",
    "\n",
    "    get_subj_metadata().to_xarray()\n",
    "])\n",
    "phi = __.pipe(\n",
    "    dict(\n",
    "        phi=data[\"phi\"],\n",
    "        std=data[\"phi\"],\n",
    "    ),\n",
    "    lambda _: xr.merge([_]).to_dataframe().reset_index()\n",
    ")\n",
    "error_line(phi, x=\"k\", y=\"phi\", err=\"std\", color=\"threshold\", markers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = xr.merge([\n",
    "    pd.concat(\n",
    "        pd.read_csv(item, index_col=0)\n",
    "        .assign(subject=int(item.entities[\"subject\"]))\n",
    "        .set_index([\"subject\", \"threshold\", \"k\"], drop=True)\n",
    "\n",
    "        for item in\n",
    "        layout_get(desc=\"avgFA\", suffix=\"richclub\")\n",
    "    ).to_xarray(),\n",
    "    get_subj_metadata().to_xarray()\n",
    "])\n",
    "grouper = data.sel(threshold=4).groupby(\"group\")\n",
    "phi = __.pipe(\n",
    "    dict(\n",
    "        mean=grouper.mean(\"subject\")[\"phi\"],\n",
    "        std=grouper.std(\"subject\")[\"phi\"],\n",
    "    ),\n",
    "    lambda _: xr.merge([_]).to_dataframe().reset_index()\n",
    ")\n",
    "error_line(phi, x=\"k\", y=\"mean\", err=\"std\", color=\"group\", markers=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31fa35b1ae39ed1b4ac1b881c358404ca5f3421144360cb0e54822825f879d61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
