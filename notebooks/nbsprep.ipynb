{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53731f-4ba6-421c-b1b2-a6dc56453792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bids import BIDSLayout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil as sh\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sklearn.preprocessing as preproc\n",
    "import sklearn.impute as impute\n",
    "import sklearn.compose as compose\n",
    "import sklearn.pipeline as pipeline\n",
    "import scipy.io\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c6974-b228-4ce5-b190-c5b0d83430cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = \"/scratch/knavynde/newtopsy\"\n",
    "out = f\"{root}/derivatives/prepdwi-recon-0.1.0\"\n",
    "layout = BIDSLayout(\n",
    "    root,\n",
    "    database_path=f\"{root}/.pybids\",\n",
    "    derivatives=True,\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc60da-4749-4709-8164-266057884c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEIGHT=\"avgFA\"\n",
    "participant_file = f'{root}/participants.tsv'\n",
    "\n",
    "def get_files():\n",
    "    return layout.get(\n",
    "        subject=get_subjects(),\n",
    "        suffix=\"connectome\",\n",
    "        desc=WEIGHT,\n",
    "        atlas=\"bn246\"\n",
    "    )\n",
    "\n",
    "def get_file_subjects():\n",
    "    return [file.get_entities()['subject'] for file in get_files()]\n",
    "\n",
    "\n",
    "def get_connectomes():\n",
    "    return np.dstack([np.loadtxt(file.path, delimiter=\",\") for file in get_files()])\n",
    "\n",
    "def get_metadata(hem = None):\n",
    "    metadata = pd.read_csv(\"resources/brainnetome.tsv\", sep=\"\\t\")\n",
    "    if hem in (\"L\", \"R\"):\n",
    "        return metadata[metadata[\"hemisphere\"] == hem].reset_index()\n",
    "    return metadata.reset_index()\n",
    "\n",
    "def get_subj_metadata():\n",
    "    df = pd.read_csv(participant_file, sep=\"\\t\")\n",
    "    participants = pd.read_csv(\n",
    "        f'{root}/derivatives/snakedwi-0.1.0/participants.tsv',\n",
    "        sep=\"\\t\",\n",
    "    )[\"participant_id\"]\n",
    "    return df[\n",
    "        lambda df: df[\"phenotype\"].isin([\"HC\", \"FEP\"])\n",
    "    ][\n",
    "        lambda df: df[\"participant_id\"].isin(participants)\n",
    "    ]\n",
    "\n",
    "def get_subjects():\n",
    "    return list(\n",
    "        get_subj_metadata()[\"participant_id\"]\n",
    "        .map(lambda subj: subj[4:])\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01468c48",
   "metadata": {},
   "source": [
    "### Connectome Data (Left and Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634e8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HEMS = (\"L\", \"R\", None)\n",
    "def get_hemispheric_connectomes():\n",
    "    connectomes = get_connectomes()\n",
    "    for hem in HEMS:\n",
    "        index = get_metadata(hem)[\"Label ID\"]\n",
    "        yield connectomes[(*np.ix_(index, index), ...)]\n",
    "\n",
    "for connectomes, hem in zip(get_hemispheric_connectomes(), HEMS):\n",
    "    base = Path(f\"{out}/nbs/{WEIGHT}_{hem or 'pan'}_connectomes\")\n",
    "    if base.exists():\n",
    "        sh.rmtree(base)\n",
    "    base.mkdir(exist_ok=True, parents=True)\n",
    "    for connectome, sub in zip(np.moveaxis(connectomes, -1, 0), get_subjects()):\n",
    "        with base.joinpath(sub).with_suffix(\".txt\").open('w') as f:\n",
    "            for line in np.matrix(connectome):\n",
    "                np.savetxt(f, line, fmt=\"%.10f\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mat file\n",
    "HEMS = (\"L\", \"R\", None)\n",
    "def get_hemispheric_connectomes():\n",
    "    connectomes = get_connectomes()\n",
    "    for hem in HEMS:\n",
    "        index = get_metadata(hem)[\"Label ID\"]\n",
    "        yield connectomes[(*np.ix_(index, index), ...)]\n",
    "\n",
    "for connectomes, hem in zip(get_hemispheric_connectomes(), HEMS):\n",
    "    scipy.io.savemat(f\"{out}/nbs/{WEIGHT}_{hem or 'pan'}_connectome.mat\", {\"Mat\": connectomes})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576ee09",
   "metadata": {},
   "source": [
    "### Node MNI Coordinates (Left and Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed3ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for hem in HEMS:\n",
    "    coords = get_metadata(hem)[\"MNI\"].str.split(\",\", expand=True).astype(int).to_numpy()\n",
    "    scipy.io.savemat(f\"{out}/nbs/coords_{hem or 'pan'}.mat\", {\"coords\": coords})\n",
    "    with Path(f\"{out}/nbs/coords_{hem or 'pan'}.txt\").open('w') as f:\n",
    "        f.write(\n",
    "            \"\\n\".join(\n",
    "                \" \".join(coord.strip().split(\", \"))\n",
    "                for coord in get_metadata(hem)[\"MNI\"]\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3d4a3",
   "metadata": {},
   "source": [
    "### Patient Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cad5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partic = get_subj_metadata()[\n",
    "    lambda df: df[\"participant_id\"].map(lambda s: s[4:]).isin(get_file_subjects())\n",
    "]\n",
    "groups = [\"HC\", \"FEP\"]\n",
    "for group in groups:\n",
    "    partic[group] = (partic['phenotype'] == group).map(int)\n",
    "\n",
    "sex_encode = pipeline.make_pipeline(\n",
    "    preproc.OrdinalEncoder(),\n",
    "    impute.SimpleImputer(strategy=\"most_frequent\")\n",
    ")\n",
    "column_preproc = compose.make_column_transformer(\n",
    "    (\"passthrough\", groups),\n",
    "    (sex_encode, [\"sex\"]),\n",
    "    (impute.SimpleImputer(), ['age']),\n",
    ")\n",
    "\n",
    "X = column_preproc.fit_transform(partic)\n",
    "scipy.io.savemat(f\"{out}/nbs/dds.mat\", {\"design\": X})\n",
    "np.savetxt(f\"{out}/nbs/ddx.txt\", X, fmt='%i')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c199526",
   "metadata": {},
   "source": [
    "### Node Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1466e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scipy.io.savemat(f\"{out}/nbs/node_labels.mat\", {\"labels\": get_metadata(\"L\")[\"Name\"].to_numpy()}, oned_as=\"column\")\n",
    "md = get_metadata()\n",
    "pan_names = md[\"Name\"] + \"_\" + md[\"hemisphere\"]\n",
    "scipy.io.savemat(f\"{out}/nbs/node_labels_pan.mat\", {\"labels\": pan_names.to_numpy()}, oned_as=\"column\")\n",
    "with Path(f\"{out}/nbs/node_labels.txt\").open(\"w\") as f:\n",
    "    # Labels the same on both hemispheres\n",
    "    f.write(\"\\n\".join(get_metadata(\"L\")[\"Name\"]) + \"\\n\")\n",
    "    \n",
    "with Path(f\"{out}/nbs/node_labels_pan.txt\").open(\"w\") as f:\n",
    "    # Labels the same on both hemispheres\n",
    "    md = get_metadata()\n",
    "    f.write(\"\\n\".join(md[\"Name\"] + \"_\" + md[\"hemisphere\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dad096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cebabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"results/nbs/atlas-bn246_weight-avgFA_model-FEP_nbs.mat\")\n",
    "results = scipy.io.loadmat(file)\n",
    "import h5py\n",
    "v = results[\"nbs\"]\n",
    "def unpack_mat(mat, scheme):\n",
    "    result = {}\n",
    "    for key, val in scheme.items():\n",
    "        if isinstance(val, dict):\n",
    "            result[key] = unpack_mat(mat[key][0,0], val)\n",
    "        elif val == \"literal\":\n",
    "            try:\n",
    "                art = mat[key][0,0].flatten()\n",
    "                if len(art):\n",
    "                    result[key] = art[0]\n",
    "                else:\n",
    "                    result[key] = ''\n",
    "            except Exception as err:\n",
    "                print(key)\n",
    "                print(mat[key])\n",
    "                raise err\n",
    "        elif val == \"arr\":\n",
    "            result[key] = mat[key][0,0]\n",
    "        else:\n",
    "            raise TypeError()\n",
    "    return result\n",
    "\n",
    "UI_STRUC = {\"ui\": \"literal\", \"ok\": \"literal\"}   \n",
    "nbs = unpack_mat(results[\"nbs\"], {\n",
    "    \"NBS\": {\n",
    "        \"n\": \"literal\",\n",
    "        \"con_mat\": \"arr\",\n",
    "        \"pval\": \"literal\",\n",
    "        \"test_stat\": \"arr\",\n",
    "    },\n",
    "    \"GLM\": {\n",
    "        \"y\": \"arr\",\n",
    "        \"X\": \"arr\",\n",
    "        \"contrast\": \"arr\",\n",
    "        \"test\": \"literal\",\n",
    "        \"perms\": \"literal\",\n",
    "    },\n",
    "    \"STATS\": {\n",
    "        \"thresh\": \"literal\",\n",
    "        \"alpha\": \"literal\",\n",
    "        \"size\": \"literal\",\n",
    "        \"N\": \"literal\",\n",
    "        \"test_stat\": \"arr\"\n",
    "    },\n",
    "    \"UI\": dict(zip([\n",
    "        \"method\",\n",
    "        \"test\",\n",
    "        \"size\",\n",
    "        \"thresh\",\n",
    "        \"perms\",\n",
    "        \"alpha\",\n",
    "        \"contrast\",\n",
    "        \"design\",\n",
    "        \"matrices\",\n",
    "        \"node_coor\",\n",
    "        \"node_label\",\n",
    "        \"exchange\",\n",
    "    ], it.repeat(UI_STRUC)))\n",
    "    # \"UI\": v[\"UI\"],\n",
    "    # \"STATS\": v[\"STATS\"],\n",
    "})\n",
    "\n",
    "# np.savetxt(file.with_suffix(\".tsv\"), nbs[\"NBS\"][\"test_stat\"], delimiter=\"\\t\")\n",
    "with h5py.File(file.with_suffix(\".hdf5\"), 'w') as f:\n",
    "    conmats = [nbs[\"NBS\"]['con_mat'][i][0].A for i in range(len(nbs[\"NBS\"]['con_mat']))]\n",
    "    conmats = np.dstack(conmats) if conmats else np.ndarray((0,))\n",
    "    NBS = f.create_group(\"nbs\")\n",
    "    NBS[\"con_mat\"] = conmats\n",
    "    NBS[\"test_stat\"] = nbs[\"NBS\"][\"test_stat\"]\n",
    "    NBS.attrs['n'] = nbs[\"NBS\"]['n']\n",
    "    NBS.attrs['pval'] = nbs[\"NBS\"]['pval']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
